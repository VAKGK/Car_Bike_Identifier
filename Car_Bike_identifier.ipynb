{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4489573-da24-4b04-95ca-eedf79e8520b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 2 classes.\n",
      "Found 798 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 1. IMPORT REQUIRED LIBRARIES\n",
    "# ==============================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# ==============================\n",
    "# 2. IMAGE PREPROCESSING\n",
    "# ==============================\n",
    "\n",
    "# This rescales image pixels from 0-255 to 0-1 (helps training)\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    rotation_range=20, \n",
    "    width_shift_range=0.2,      # Horizontal shift\n",
    "    height_shift_range=0.2,     # Vertical shift\n",
    "    shear_range=0.2,            # Shear transformation\n",
    "    zoom_range=0.4,             # Random zoom\n",
    "    horizontal_flip=True,       # Flip images horizontally\n",
    "    fill_mode='nearest'         # Fill new pixels after transformation\n",
    "    )\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    rotation_range=20,          # Random rotation ±20 degrees\n",
    "    width_shift_range=0.2,      # Horizontal shift\n",
    "    height_shift_range=0.2,     # Vertical shift\n",
    "    shear_range=0.2,            # Shear transformation\n",
    "    zoom_range=0.4,             # Random zoom\n",
    "    horizontal_flip=True,       # Flip images horizontally\n",
    "    fill_mode='nearest'         # Fill new pixels after transformation\n",
    "    )\n",
    "\n",
    "# Load training images\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    r\"D:\\DATA SCIENCE\\DL\\PROJECTS\\car_bike\\Car-Bike-Dataset\\Training_Set\",      # path to training folder\n",
    "    target_size=(128,128), # resize all images to 64x64\n",
    "    batch_size=32,\n",
    "    class_mode='binary'   # car vs bike (2 classes)\n",
    ")\n",
    "\n",
    "# Load testing images\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    r\"D:\\DATA SCIENCE\\DL\\PROJECTS\\car_bike\\Car-Bike-Dataset\\Test_Set\",\n",
    "    target_size=(128,128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 3. BUILD CNN MODEL\n",
    "# ==============================\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# ---- Convolution Layer 1 ----\n",
    "# Learns edges and simple shapes\n",
    "model.add(Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3,3),\n",
    "    activation='relu',\n",
    "    input_shape=(128,128,3)\n",
    "))\n",
    "\n",
    "# Conv Layer 2 (NEW)\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Conv Layer 3 (NEW)\n",
    "model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# ---- Convolution Layer 2 ----\n",
    "# Learns more complex features\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# ---- Flatten ----\n",
    "# Converts 2D feature maps into 1D vector\n",
    "model.add(Flatten())\n",
    "\n",
    "# ---- Fully Connected Layer ----\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# ---- Output Layer ----\n",
    "# 1 neuron because binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# ==============================\n",
    "# 4. COMPILE THE MODEL\n",
    "# ==============================\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',              # controls learning speed\n",
    "    loss='binary_crossentropy',    # for 2 classes\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfc2960e-ae80-454e-9812-303783ee4651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m 17/125\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:00\u001b[0m 2s/step - accuracy: 0.5724 - loss: 1.5220 - precision: 0.6050 - recall: 0.5567"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP ELITE BOOK\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 2s/step - accuracy: 0.6973 - loss: 0.9682 - precision: 0.7030 - recall: 0.6830 - val_accuracy: 0.4987 - val_loss: 1.1662 - val_precision: 0.4987 - val_recall: 1.0000\n",
      "Epoch 2/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 2s/step - accuracy: 0.7945 - loss: 0.5455 - precision: 0.8011 - recall: 0.7835 - val_accuracy: 0.5013 - val_loss: 3.5190 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 2s/step - accuracy: 0.8418 - loss: 0.4122 - precision: 0.8379 - recall: 0.8475 - val_accuracy: 0.5113 - val_loss: 0.8843 - val_precision: 0.5112 - val_recall: 0.4598\n",
      "Epoch 4/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1s/step - accuracy: 0.8692 - loss: 0.3158 - precision: 0.8672 - recall: 0.8720 - val_accuracy: 0.6930 - val_loss: 0.5977 - val_precision: 0.6349 - val_recall: 0.9045\n",
      "Epoch 5/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 1s/step - accuracy: 0.8790 - loss: 0.3005 - precision: 0.8782 - recall: 0.8800 - val_accuracy: 0.6704 - val_loss: 0.7197 - val_precision: 0.6021 - val_recall: 1.0000\n",
      "Epoch 6/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 1s/step - accuracy: 0.8950 - loss: 0.2649 - precision: 0.8892 - recall: 0.9025 - val_accuracy: 0.8985 - val_loss: 0.2289 - val_precision: 0.9204 - val_recall: 0.8719\n",
      "Epoch 7/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 1s/step - accuracy: 0.9007 - loss: 0.2534 - precision: 0.8986 - recall: 0.9035 - val_accuracy: 0.6629 - val_loss: 1.0732 - val_precision: 1.0000 - val_recall: 0.3241\n",
      "Epoch 8/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 1s/step - accuracy: 0.9095 - loss: 0.2175 - precision: 0.9071 - recall: 0.9125 - val_accuracy: 0.8446 - val_loss: 0.3970 - val_precision: 0.9858 - val_recall: 0.6985\n",
      "Epoch 9/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 1s/step - accuracy: 0.9160 - loss: 0.2093 - precision: 0.9143 - recall: 0.9180 - val_accuracy: 0.8810 - val_loss: 0.2887 - val_precision: 0.9749 - val_recall: 0.7814\n",
      "Epoch 10/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 1s/step - accuracy: 0.9197 - loss: 0.2108 - precision: 0.9162 - recall: 0.9240 - val_accuracy: 0.9123 - val_loss: 0.2170 - val_precision: 0.8710 - val_recall: 0.9673\n",
      "Epoch 11/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 2s/step - accuracy: 0.9205 - loss: 0.2055 - precision: 0.9180 - recall: 0.9235 - val_accuracy: 0.9035 - val_loss: 0.2384 - val_precision: 0.8527 - val_recall: 0.9749\n",
      "Epoch 12/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 2s/step - accuracy: 0.9333 - loss: 0.1723 - precision: 0.9330 - recall: 0.9335 - val_accuracy: 0.9273 - val_loss: 0.1864 - val_precision: 0.9009 - val_recall: 0.9598\n",
      "Epoch 13/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m658s\u001b[0m 5s/step - accuracy: 0.9280 - loss: 0.1850 - precision: 0.9263 - recall: 0.9300 - val_accuracy: 0.9361 - val_loss: 0.1713 - val_precision: 0.9483 - val_recall: 0.9221\n",
      "Epoch 14/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 2s/step - accuracy: 0.9317 - loss: 0.1768 - precision: 0.9307 - recall: 0.9330 - val_accuracy: 0.9386 - val_loss: 0.1642 - val_precision: 0.9580 - val_recall: 0.9171\n",
      "Epoch 15/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 2s/step - accuracy: 0.9300 - loss: 0.1796 - precision: 0.9274 - recall: 0.9330 - val_accuracy: 0.8183 - val_loss: 0.6654 - val_precision: 0.9884 - val_recall: 0.6432\n",
      "Epoch 16/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 2s/step - accuracy: 0.9337 - loss: 0.1637 - precision: 0.9255 - recall: 0.9435 - val_accuracy: 0.9273 - val_loss: 0.1961 - val_precision: 0.9620 - val_recall: 0.8894\n",
      "Epoch 17/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 2s/step - accuracy: 0.9398 - loss: 0.1557 - precision: 0.9343 - recall: 0.9460 - val_accuracy: 0.9536 - val_loss: 0.1274 - val_precision: 0.9413 - val_recall: 0.9673\n",
      "Epoch 18/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 2s/step - accuracy: 0.9413 - loss: 0.1430 - precision: 0.9410 - recall: 0.9415 - val_accuracy: 0.9461 - val_loss: 0.1559 - val_precision: 0.9810 - val_recall: 0.9095\n",
      "Epoch 19/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 2s/step - accuracy: 0.9402 - loss: 0.1488 - precision: 0.9374 - recall: 0.9435 - val_accuracy: 0.9023 - val_loss: 0.2571 - val_precision: 0.9848 - val_recall: 0.8166\n",
      "Epoch 20/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 2s/step - accuracy: 0.9435 - loss: 0.1423 - precision: 0.9431 - recall: 0.9440 - val_accuracy: 0.8559 - val_loss: 0.4302 - val_precision: 0.9829 - val_recall: 0.7236\n",
      "Epoch 21/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 2s/step - accuracy: 0.9490 - loss: 0.1341 - precision: 0.9468 - recall: 0.9515 - val_accuracy: 0.9085 - val_loss: 0.2157 - val_precision: 0.8556 - val_recall: 0.9824\n",
      "Epoch 22/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 2s/step - accuracy: 0.9492 - loss: 0.1308 - precision: 0.9455 - recall: 0.9535 - val_accuracy: 0.8622 - val_loss: 0.3761 - val_precision: 0.9932 - val_recall: 0.7286\n",
      "Epoch 23/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 2s/step - accuracy: 0.9505 - loss: 0.1350 - precision: 0.9514 - recall: 0.9495 - val_accuracy: 0.9599 - val_loss: 0.0980 - val_precision: 0.9598 - val_recall: 0.9598\n",
      "Epoch 24/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5515s\u001b[0m 44s/step - accuracy: 0.9513 - loss: 0.1296 - precision: 0.9479 - recall: 0.9550 - val_accuracy: 0.8684 - val_loss: 0.5405 - val_precision: 0.9933 - val_recall: 0.7412\n",
      "Epoch 25/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 1s/step - accuracy: 0.9532 - loss: 0.1273 - precision: 0.9548 - recall: 0.9515 - val_accuracy: 0.7005 - val_loss: 1.0669 - val_precision: 1.0000 - val_recall: 0.3995\n",
      "Epoch 26/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 1s/step - accuracy: 0.9505 - loss: 0.1288 - precision: 0.9514 - recall: 0.9495 - val_accuracy: 0.9461 - val_loss: 0.1281 - val_precision: 0.9404 - val_recall: 0.9523\n",
      "Epoch 27/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 1s/step - accuracy: 0.9445 - loss: 0.1359 - precision: 0.9405 - recall: 0.9490 - val_accuracy: 0.9474 - val_loss: 0.1469 - val_precision: 0.9759 - val_recall: 0.9171\n",
      "Epoch 28/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 1s/step - accuracy: 0.9567 - loss: 0.1163 - precision: 0.9565 - recall: 0.9570 - val_accuracy: 0.9511 - val_loss: 0.1203 - val_precision: 0.9305 - val_recall: 0.9749\n",
      "Epoch 29/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 1s/step - accuracy: 0.9565 - loss: 0.1184 - precision: 0.9511 - recall: 0.9625 - val_accuracy: 0.9073 - val_loss: 0.2438 - val_precision: 0.9969 - val_recall: 0.8166\n",
      "Epoch 30/30\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 1s/step - accuracy: 0.9530 - loss: 0.1118 - precision: 0.9539 - recall: 0.9520 - val_accuracy: 0.8346 - val_loss: 0.5792 - val_precision: 0.9963 - val_recall: 0.6709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x219fdb04cd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. TRAIN THE MODEL\n",
    "# ==============================\n",
    "\n",
    "model.fit(\n",
    "    train_data,\n",
    "    epochs=30,          # increase for better accuracy\n",
    "    validation_data=test_data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b283c5d9-a722-4dd0-b64a-a98c64741974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 6. SAVE THE MODEL\n",
    "# ==============================\n",
    "\n",
    "model.save(\"car_bike_cnn_model.h5\")\n",
    "\n",
    "print(\"Model training complete and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f861e27-b456-4f00-86bd-f4ade91b7f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Bike\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "img = image.load_img(r\"C:\\Users\\HP ELITE BOOK\\Downloads\\pexels-avinashpatel-445399.jpg\", target_size=(128,128))\n",
    "img = image.img_to_array(img)\n",
    "img = img / 255.0\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "prediction = model.predict(img)\n",
    "\n",
    "pred = model.predict(img)[0][0]\n",
    "\n",
    "if pred > 0.80:\n",
    "    print(\"Car\")\n",
    "elif pred < 0.20:\n",
    "    print(\"Bike\")\n",
    "else:\n",
    "    print(\"Unknown Image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b8cf4-6cf6-48c5-b9f2-01f45faed0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "results = model.evaluate(test_data)\n",
    "\n",
    "# Print metric names and their values\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(f\"{name}: {value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
